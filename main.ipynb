import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
from urllib.error import URLError
from scipy.stats import norm, laplace, entropy  # <-- Added entropy

def generate_synthetic_data(n_samples=500, n_features=10, noise=15.0, random_state=42):
    """
    Generates a synthetic dataset for linear regression.

    Returns:
        X_scaled (np.array): Scaled feature matrix
        y (np.array): Target vector
        true_coef (np.array): The true coefficients used to generate the data
    """
    print(f"Generating synthetic data: {n_samples} samples, {n_features} features, noise={noise}")
    X, y, true_coef = make_regression(
        n_samples=n_samples,
        n_features=n_features,
        n_informative=n_features,  # All features are informative
        noise=noise,
        coef=True,
        random_state=random_state
    )

    # Scale the features, which is good practice
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return X_scaled, y, true_coef

def load_boston_data():
    """
    Loads and preprocesses the Boston Housing dataset using the provided snippet.

    Returns:
        X_boston (np.array): Scaled feature matrix or None if loading fails
        y_boston (np.array): Target vector or None if loading fails
    """
    print("Loading Boston Housing dataset...")
    data_url = "http://lib.stat.cmu.edu/datasets/boston"

    try:
        raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
        target = raw_df.values[1::2, 2]

        scaler = StandardScaler()
        X_boston = scaler.fit_transform(data)
        y_boston = target

        print(f"Boston data loaded successfully. Shape: {X_boston.shape}")
        return X_boston, y_boston

    except (URLError, pd.errors.ParserError, Exception) as e:
        print(f"Error loading or processing Boston data: {e}")
        print("Please check your internet connection or the data URL.")
        return None, None

def train_and_analyze_models(X, y, n_models=500, min_frac=0.5, max_frac=0.9):
    """
    Trains n_models on random, variable-sized subsets of X and y.
    Performs PCA on the learned weights.

    Args:
        X (np.array): Full feature matrix
        y (np.array): Full target vector
        n_models (int): Number of models to train
        min_frac (float): Minimum fraction of data for a subset
        max_frac (float): Maximum fraction of data for a subset

    Returns:
        weights_array (np.array): (n_models, n_features) array of learned weights
        weights_pca (np.array): (n_models, 2) array of PCA-transformed weights
    """
    if X is None or y is None:
        return None, None

    n_samples_total, n_features = X.shape
    all_weights = []

    print(f"Training {n_models} models on random subsets...")

    for _ in range(n_models):
        # 1. Determine a random subset size
        subset_frac = np.random.uniform(min_frac, max_frac)
        subset_size = int(subset_frac * n_samples_total)

        # Ensure subset is at least large enough to solve the system (n_features + 1)
        subset_size = max(n_features + 1, subset_size)

        # 2. Get random indices for the subset
        indices = np.random.choice(n_samples_total, size=subset_size, replace=False)

        # 3. Create the subset
        X_subset = X[indices]
        y_subset = y[indices]

        # 4. Train the model
        model = LinearRegression()
        model.fit(X_subset, y_subset)

        # 5. Store the learned weights (coefficients)
        all_weights.append(model.coef_)

    weights_array = np.array(all_weights)

    print("Performing PCA on learned weights...")
    # 6. Perform PCA on the weights
    pca = PCA(n_components=2)
    weights_pca = pca.fit_transform(weights_array)

    return weights_array, weights_pca

def plot_distributions(weights_pca, title):
    """
    Generates histograms (frequency distributions) for the
    first and second principal components of the weights and
    compares them to Gaussian and Laplacian fits.
    """
    if weights_pca is None:
        print(f"Skipping plot for '{title}' due to missing data.")
        return

    pc1_data = weights_pca[:, 0]
    pc2_data = weights_pca[:, 1]

    # --- 1. Calculate Statistics ---
    mu1, var1 = np.mean(pc1_data), np.var(pc1_data)
    std1 = np.sqrt(var1)
    mu2, var2 = np.mean(pc2_data), np.var(pc2_data)
    std2 = np.sqrt(var2)

    # For Laplacian, variance = 2 * (scale ** 2). So scale = sqrt(variance / 2) = std / sqrt(2)
    laplace_scale1 = std1 / np.sqrt(2)
    laplace_scale2 = std2 / np.sqrt(2)

    print(f"\n--- Statistics for {title} ---")
    print(f"PC1: Mean = {mu1:.4f}, Variance = {var1:.4f}, StdDev = {std1:.4f}")
    print(f"PC2: Mean = {mu2:.4f}, Variance = {var2:.4f}, StdDev = {std2:.4f}")

    # --- 4. Calculate KL Divergence ---
    # We need to discretize both distributions into the same bins
    # P = Empirical Data, Q = Theoretical (Gaussian/Laplacian)
    # We will calculate KL(P || Q) using scipy.stats.entropy(pk, qk)

    N_BINS = 30 # Use the same number of bins as the plots
    epsilon = 1e-10 # To avoid division by zero

    # --- PC1 KL Calculations ---
    # 1. Get P (Empirical)
    counts1, bin_edges1 = np.histogram(pc1_data, bins=N_BINS)
    pk1 = counts1 / np.sum(counts1)

    # 2. Get Q (Gaussian)
    cdf_gauss1 = norm.cdf(bin_edges1, mu1, std1)
    qk_gauss1 = np.diff(cdf_gauss1) + epsilon # Add epsilon to all bins
    qk_gauss1 /= np.sum(qk_gauss1) # Re-normalize

    # 3. Get Q (Laplacian)
    cdf_laplace1 = laplace.cdf(bin_edges1, mu1, laplace_scale1)
    qk_laplace1 = np.diff(cdf_laplace1) + epsilon # Add epsilon to all bins
    qk_laplace1 /= np.sum(qk_laplace1) # Re-normalize

    # 4. Calculate KL
    kl_gauss1 = entropy(pk1, qk_gauss1)
    kl_laplace1 = entropy(pk1, qk_laplace1)

    # --- PC2 KL Calculations ---
    # 1. Get P (Empirical)
    counts2, bin_edges2 = np.histogram(pc2_data, bins=N_BINS)
    pk2 = counts2 / np.sum(counts2)

    # 2. Get Q (Gaussian)
    cdf_gauss2 = norm.cdf(bin_edges2, mu2, std2)
    qk_gauss2 = np.diff(cdf_gauss2) + epsilon
    qk_gauss2 /= np.sum(qk_gauss2)

    # 3. Get Q (Laplacian)
    cdf_laplace2 = laplace.cdf(bin_edges2, mu2, laplace_scale2)
    qk_laplace2 = np.diff(cdf_laplace2) + epsilon
    qk_laplace2 /= np.sum(qk_laplace2)

    # 4. Calculate KL
    kl_gauss2 = entropy(pk2, qk_gauss2)
    kl_laplace2 = entropy(pk2, qk_laplace2)

    # --- 5. Print KL Results ---
    print(f"PC1 KL(Data || Gaussian):  {kl_gauss1:.4f}")
    print(f"PC1 KL(Data || Laplacian): {kl_laplace1:.4f}")
    print(f"PC2 KL(Data || Gaussian):  {kl_gauss2:.4f}")
    print(f"PC2 KL(Data || Laplacian): {kl_laplace2:.4f}")

    # --- 2. Create data for plotting PDFs ---
    # Create a smooth x-axis for plotting the PDF curves
    x1 = np.linspace(pc1_data.min(), pc1_data.max(), 200)
    x2 = np.linspace(pc2_data.min(), pc2_data.max(), 200)

    # Generate the PDF values
    gauss_pdf1 = norm.pdf(x1, mu1, std1)
    gauss_pdf2 = norm.pdf(x2, mu2, std2)

    laplace_pdf1 = laplace.pdf(x1, mu1, laplace_scale1)
    laplace_pdf2 = laplace.pdf(x2, mu2, laplace_scale2)

    # --- 3. Plotting ---

    # Plot 1: Original Frequency Histogram (as requested)
    fig1, (ax1a, ax1b) = plt.subplots(1, 2, figsize=(16, 6))
    fig1.suptitle(f"Frequency Distribution ({title})", fontsize=18, y=1.03)

    ax1a.hist(pc1_data, bins=30, color='blue', alpha=0.7, edgecolor='black')
    ax1a.set_title(f"PC1 Frequency", fontsize=14)
    ax1a.set_xlabel("PC1 Value", fontsize=12)
    ax1a.set_ylabel("Frequency", fontsize=12)
    ax1a.grid(True, linestyle='--', alpha=0.6)

    ax1b.hist(pc2_data, bins=30, color='green', alpha=0.7, edgecolor='black')
    ax1b.set_title(f"PC2 Frequency", fontsize=14)
    ax1b.set_xlabel("PC2 Value", fontsize=12)
    ax1b.set_ylabel("Frequency", fontsize=12)
    ax1b.grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout()

    # Plot 2: Gaussian Comparison (Density)
    fig2, (ax2a, ax2b) = plt.subplots(1, 2, figsize=(16, 6))
    fig2.suptitle(f"Gaussian PDF Comparison ({title})", fontsize=18, y=1.03)

    # Plot PC1 vs Gaussian
    ax2a.hist(pc1_data, bins=30, color='blue', alpha=0.7, edgecolor='black', density=True, label='Data Histogram')
    ax2a.plot(x1, gauss_pdf1, 'r-', linewidth=3, label=f'Gaussian PDF\n($\mu={mu1:.2f}, \sigma^2={var1:.2f}$)')
    ax2a.set_title(f"PC1 vs. Gaussian", fontsize=14)
    ax2a.set_xlabel("PC1 Value", fontsize=12)
    ax2a.set_ylabel("Density", fontsize=12)
    ax2a.grid(True, linestyle='--', alpha=0.6)
    ax2a.legend()

    # Plot PC2 vs Gaussian
    ax2b.hist(pc2_data, bins=30, color='green', alpha=0.7, edgecolor='black', density=True, label='Data Histogram')
    ax2b.plot(x2, gauss_pdf2, 'r-', linewidth=3, label=f'Gaussian PDF\n($\mu={mu2:.2f}, \sigma^2={var2:.2f}$)')
    ax2b.set_title(f"PC2 vs. Gaussian", fontsize=14)
    ax2b.set_xlabel("PC2 Value", fontsize=12)
    ax2b.set_ylabel("Density", fontsize=12)
    ax2b.grid(True, linestyle='--', alpha=0.6)
    ax2b.legend()

    plt.tight_layout()

    # Plot 3: Laplacian Comparison (Density)
    fig3, (ax3a, ax3b) = plt.subplots(1, 2, figsize=(16, 6))
    fig3.suptitle(f"Laplacian PDF Comparison ({title})", fontsize=18, y=1.03)

    # Plot PC1 vs Laplacian
    ax3a.hist(pc1_data, bins=30, color='blue', alpha=0.7, edgecolor='black', density=True, label='Data Histogram')
    ax3a.plot(x1, laplace_pdf1, 'm-', linewidth=3, label=f'Laplacian PDF\n($\mu={mu1:.2f}, b={laplace_scale1:.2f}$)')
    ax3a.set_title(f"PC1 vs. Laplacian", fontsize=14)
    ax3a.set_xlabel("PC1 Value", fontsize=12)
    ax3a.set_ylabel("Density", fontsize=12)
    ax3a.grid(True, linestyle='--', alpha=0.6)
    ax3a.legend()

    # Plot PC2 vs Laplacian
    ax3b.hist(pc2_data, bins=30, color='green', alpha=0.7, edgecolor='black', density=True, label='Data Histogram')
    ax3b.plot(x2, laplace_pdf2, 'm-', linewidth=3, label=f'Laplacian PDF\n($\mu={mu2:.2f}, b={laplace_scale2:.2f}$)')
    ax3b.set_title(f"PC2 vs. Laplacian", fontsize=14)
    ax3b.set_xlabel("PC2 Value", fontsize=12)
    ax3b.set_ylabel("Density", fontsize=12)
    ax3b.grid(True, linestyle='--', alpha=0.6)
    ax3b.legend()

    plt.tight_layout()

    # Show all generated plots
    plt.show()

if __name__ == "__main__":
    # --- Parameters ---
    N_MODELS = 1000          # Number of models to train for each case
    MIN_SUBSET_FRAC = 0.2   # Min % of data in a random subset
    MAX_SUBSET_FRAC = 0.8   # Max % of data in a random subset

    # --- Case 1: Synthetic Data ---
    print("--- Running Synthetic Data Case ---")
    X_synth, y_synth, true_coef = generate_synthetic_data(
        n_samples=1000,
        n_features=15,
        noise=25.0
    )
    print(f"True coefficients (first 5): {true_coef[:5]}")

    weights_synth, pca_synth = train_and_analyze_models(
        X_synth, y_synth, N_MODELS, MIN_SUBSET_FRAC, MAX_SUBSET_FRAC
    )

    if weights_synth is not None:
        print(f"Shape of collected weights: {weights_synth.shape}")
        plot_distributions(  # <-- Renamed function call
            pca_synth,
            "Synthetic Data"
        )

    # --- Case 2: Boston Housing Data ---
    print("\n" + "--- Running Boston Housing Data Case ---")
    X_boston, y_boston = load_boston_data()

    if X_boston is not None:
        weights_boston, pca_boston = train_and_analyze_models(
            X_boston, y_boston, N_MODELS, MIN_SUBSET_FRAC, MAX_SUBSET_FRAC
        )

        if weights_boston is not None:
            print(f"Shape of collected weights: {weights_boston.shape}")
            plot_distributions(  # <-- Renamed function call
                pca_boston,
                "Boston Housing Data"
            )
    else:
        print("Skipping Boston Housing analysis due to loading failure.")

    print("\nAnalysis complete.")
